{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"provenance":[{"file_id":"1Q2He1cJbF1UpUa2DbsQQj9TrZVwKrqBH","timestamp":1665336561214}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"wazQdghw1h2_"},"source":["<center>\n","    <h3>University of Toronto</h3>\n","    <h3>Department of Mechanical and Industrial Engineering</h3>\n","    <h3>MIE368 Analytics in Action </h3>\n","    <h3>(Fall 2022)</h3>\n","    <hr>\n","    <h1>Lab 6: Model Engineering</h1>\n","    <h3>October 26, 2022</h3>\n","</center>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"75iHeAGv1h3C"},"source":["# Introduction\n","\n","We have implemented several models throughout this course and  discussed when one is more appropriate than the other. However, often times in practice there are no clear guidelines on how to develop the best model for an application. Suppose we want to build a binary classifier with high recall and precision. How should we proceed if our off-the-shelf model performs poorly on the dataset?\n","\n","In this lab, we will explore a series of techniques for improving your model and features. We will cover seven techniques:\n","\n","- **Model selection**: Try different models to see what works best\n","- **Scaling**: The scale of the values in your data is important\n","- **Feature engineering**: If the existing features aren't useful then we can create new and more relevant features   \n","- **Feature selection**: If you have too many features then we can remove some that aren't useful\n","- **Grid Search**: A process for finding a set of good hyper-parameters (e.g., regularization strength)\n","- **Model stacking**: A process where the output from one or more models are input into another\n","- **Bagging**: A process for combining models that involves combining the outputs from multiple models\n","\n","Model engineering is more of an art than it is a science. These techniques will work better in some instances than they do in others, and sometimes these techniques can actually make a model perform worse! When you do model engineering it really boils does to doing trial-and-error with these techniques. It is possible to combine these techniques (e.g., do model selection, scaling, and grid search), however, to stop this lab from becoming convoluted you will only combine each technique with model selection.\n","\n","Please keep in mind that these are just a set of possible ways to improve our models. There are many ways to engineer better models, and often the practice of model engineering comes down to the application and our prior experience. We emphasize that in this lab these techniques are largely applied in isolation, but we encourage you to combine multiple techniques in your project."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Ne5Do4xAIue5"},"source":["## Application\n","This lab will focus on the lending industry, where investors provide loans to borrowers in exchange for the promise of repayment with interest. If the borrower repays the loan, then the lender profits from the interest. However, sometimes the borrower is unable to repay the loan, meaning the lender loses money. The lender in this problem wants to predict if a borrower is unlikely to repay a loan and has asked us to build the best possible prediction model.\n","\n","First, let us import the essential tools."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"hw1OAVAnIue6"},"source":["# Standard analytics packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","# Import useful packages from sklearn\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Q-kQo0hEIue-"},"source":["Next, we'll load the dataset for this lab and store it as `df_raw`."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"cLKPAHPZIue-","colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"status":"ok","timestamp":1667225539743,"user_tz":240,"elapsed":965,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"c21eb120-4469-40ad-cf97-a28443ecf3f4"},"source":["# Import data for lab\n","df_raw = pd.read_csv(\"https://docs.google.com/uc?export=download&id=1zj7qSc5mjgXLjQgOfF60PsbO2wupLnTm\")\n","df_raw.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   CreditPolicy             Purpose  IntRate  Installment    Dti  Fico  \\\n","0             1  debt_consolidation   0.1189       829.10  19.48   737   \n","1             1         credit_card   0.1071       228.22  14.29   707   \n","2             1  debt_consolidation   0.1357       366.86  11.63   682   \n","3             1  debt_consolidation   0.1008       162.34   8.10   712   \n","4             1         credit_card   0.1426       102.92  14.97   667   \n","\n","   DaysWithCrLine  RevolBal  RevolUtil  InqLast6mths  Delinq2yrs  PubRec  \\\n","0     5639.958333     28854       52.1             0           0       0   \n","1     2760.000000     33623       76.7             0           0       0   \n","2     4710.000000      3511       25.6             1           0       0   \n","3     2699.958333     33667       73.2             1           0       0   \n","4     4066.000000      4740       39.5             0           1       0   \n","\n","   NotFullyPaid     AnnualInc  \n","0             0  85000.000385  \n","1             0  65000.000073  \n","2             0  31999.999943  \n","3             0  85000.000385  \n","4             0  80799.999636  "],"text/html":["\n","  <div id=\"df-16ea39e7-3d05-4b13-840e-9d9823361200\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditPolicy</th>\n","      <th>Purpose</th>\n","      <th>IntRate</th>\n","      <th>Installment</th>\n","      <th>Dti</th>\n","      <th>Fico</th>\n","      <th>DaysWithCrLine</th>\n","      <th>RevolBal</th>\n","      <th>RevolUtil</th>\n","      <th>InqLast6mths</th>\n","      <th>Delinq2yrs</th>\n","      <th>PubRec</th>\n","      <th>NotFullyPaid</th>\n","      <th>AnnualInc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>debt_consolidation</td>\n","      <td>0.1189</td>\n","      <td>829.10</td>\n","      <td>19.48</td>\n","      <td>737</td>\n","      <td>5639.958333</td>\n","      <td>28854</td>\n","      <td>52.1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>credit_card</td>\n","      <td>0.1071</td>\n","      <td>228.22</td>\n","      <td>14.29</td>\n","      <td>707</td>\n","      <td>2760.000000</td>\n","      <td>33623</td>\n","      <td>76.7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>65000.000073</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>debt_consolidation</td>\n","      <td>0.1357</td>\n","      <td>366.86</td>\n","      <td>11.63</td>\n","      <td>682</td>\n","      <td>4710.000000</td>\n","      <td>3511</td>\n","      <td>25.6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>31999.999943</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>debt_consolidation</td>\n","      <td>0.1008</td>\n","      <td>162.34</td>\n","      <td>8.10</td>\n","      <td>712</td>\n","      <td>2699.958333</td>\n","      <td>33667</td>\n","      <td>73.2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>credit_card</td>\n","      <td>0.1426</td>\n","      <td>102.92</td>\n","      <td>14.97</td>\n","      <td>667</td>\n","      <td>4066.000000</td>\n","      <td>4740</td>\n","      <td>39.5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>80799.999636</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16ea39e7-3d05-4b13-840e-9d9823361200')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-16ea39e7-3d05-4b13-840e-9d9823361200 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-16ea39e7-3d05-4b13-840e-9d9823361200');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"VMKE0_qiIufB"},"source":["The data if `df_raw` is described in the following data dictionary:\n","\n","|Columns         |Definition                               |\n","|:--------------:|:----------------------------------------|\n","|**Features**    |  |\n","|CreditPolicy    |1 if the borrower meets the underwriting criteria                       |\n","|Purpose         |Written purpose of the loan                           |    \n","|IntRate         |Assigned interest rate                           |\n","|Installment     |Monthly installments                      |  \n","|Dti             |Borrower's debt to income ratio               |\n","|Fico            |Borrower's FICO (credit) score                    |\n","|DaysWithCrLine  |Number of days the borrower has had an existing line of credit                 |\n","|RevolBal        |Revolving balance (current unpaid credit-card debt)            |\n","|RevolUtil       |Revolving line utilization (fraction of line of credit used)                 |\n","|InqLast5mths    |Number of inquiries by creditors in last 5 months            |\n","|Delinq2yrs      |Number of times the borrower has been 30+days past due in last 2 years                         |\n","|PubReq          |Number of derogatory public records                    |\n","|AnnualInc       |Self-reported annual income of borrower         |\n","|**Target**      |                                 |\n","|NotFullyPaid    |1 if the loan was not fully paid back                       |\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"pasUct3AIufC"},"source":["# Exploratory data analysis and cleaning\n","In this section, we'll quickly clean the data so that all categorical information is encoded as a series of binary values, and then we'll do some brief exploratory data analysis (EDA) to better understand the data."]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"DNb8GQJAIufC"},"source":["## Data Cleaning\n","Note that the dataset is already quite clean (e.g., no missing values). The only real issue with it right now is that the \"Purpose\" column in `df_raw` is categorical. Let's encode the information into a series of binary variables. Note, that encoding values as zeros and ones is often called _one-hot encoding_ in the analytics community, however, it also goes by _making dummy variables_ in the statistics community. We will use the term one-hot encoding to be consistent with the analytics community, however, `pandas` (and some other popular packages) use the term \"dummies\"."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"8Eprz59_IufD","colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"status":"ok","timestamp":1667225539743,"user_tz":240,"elapsed":5,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"250e02dc-b57a-4fe1-81bd-d86de9ea4ca0"},"source":["# Convert categorical data into \"dummy\" variables\n","df = pd.get_dummies(df_raw, columns=['Purpose']) \n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   CreditPolicy  IntRate  Installment    Dti  Fico  DaysWithCrLine  RevolBal  \\\n","0             1   0.1189       829.10  19.48   737     5639.958333     28854   \n","1             1   0.1071       228.22  14.29   707     2760.000000     33623   \n","2             1   0.1357       366.86  11.63   682     4710.000000      3511   \n","3             1   0.1008       162.34   8.10   712     2699.958333     33667   \n","4             1   0.1426       102.92  14.97   667     4066.000000      4740   \n","\n","   RevolUtil  InqLast6mths  Delinq2yrs  PubRec  NotFullyPaid     AnnualInc  \\\n","0       52.1             0           0       0             0  85000.000385   \n","1       76.7             0           0       0             0  65000.000073   \n","2       25.6             1           0       0             0  31999.999943   \n","3       73.2             1           0       0             0  85000.000385   \n","4       39.5             0           1       0             0  80799.999636   \n","\n","   Purpose_all_other  Purpose_credit_card  Purpose_debt_consolidation  \\\n","0                  0                    0                           1   \n","1                  0                    1                           0   \n","2                  0                    0                           1   \n","3                  0                    0                           1   \n","4                  0                    1                           0   \n","\n","   Purpose_educational  Purpose_home_improvement  Purpose_major_purchase  \\\n","0                    0                         0                       0   \n","1                    0                         0                       0   \n","2                    0                         0                       0   \n","3                    0                         0                       0   \n","4                    0                         0                       0   \n","\n","   Purpose_small_business  \n","0                       0  \n","1                       0  \n","2                       0  \n","3                       0  \n","4                       0  "],"text/html":["\n","  <div id=\"df-e538301c-c018-4516-ba26-221ec03ddf2f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditPolicy</th>\n","      <th>IntRate</th>\n","      <th>Installment</th>\n","      <th>Dti</th>\n","      <th>Fico</th>\n","      <th>DaysWithCrLine</th>\n","      <th>RevolBal</th>\n","      <th>RevolUtil</th>\n","      <th>InqLast6mths</th>\n","      <th>Delinq2yrs</th>\n","      <th>PubRec</th>\n","      <th>NotFullyPaid</th>\n","      <th>AnnualInc</th>\n","      <th>Purpose_all_other</th>\n","      <th>Purpose_credit_card</th>\n","      <th>Purpose_debt_consolidation</th>\n","      <th>Purpose_educational</th>\n","      <th>Purpose_home_improvement</th>\n","      <th>Purpose_major_purchase</th>\n","      <th>Purpose_small_business</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.1189</td>\n","      <td>829.10</td>\n","      <td>19.48</td>\n","      <td>737</td>\n","      <td>5639.958333</td>\n","      <td>28854</td>\n","      <td>52.1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.1071</td>\n","      <td>228.22</td>\n","      <td>14.29</td>\n","      <td>707</td>\n","      <td>2760.000000</td>\n","      <td>33623</td>\n","      <td>76.7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>65000.000073</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0.1357</td>\n","      <td>366.86</td>\n","      <td>11.63</td>\n","      <td>682</td>\n","      <td>4710.000000</td>\n","      <td>3511</td>\n","      <td>25.6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>31999.999943</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0.1008</td>\n","      <td>162.34</td>\n","      <td>8.10</td>\n","      <td>712</td>\n","      <td>2699.958333</td>\n","      <td>33667</td>\n","      <td>73.2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.1426</td>\n","      <td>102.92</td>\n","      <td>14.97</td>\n","      <td>667</td>\n","      <td>4066.000000</td>\n","      <td>4740</td>\n","      <td>39.5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>80799.999636</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e538301c-c018-4516-ba26-221ec03ddf2f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e538301c-c018-4516-ba26-221ec03ddf2f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e538301c-c018-4516-ba26-221ec03ddf2f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"wVioAlyCIufF"},"source":["Throughout this lab we use a training set (i.e., `X_train` and `y_train`) and testing set (e.g., `X_test` and `y_test`).\n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"I7jVeR-fIufG"},"source":["'''Make training and testing dataset that we'll use throughout the lab'''\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['NotFullyPaid']),\n","                                                    df.NotFullyPaid,\n","                                                    test_size = 0.3,\n","                                                    random_state = 1)\n","\n","# Make a dataframe for the training data \n","df_train = X_train.join(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"qc6LRDsOIufI"},"source":["We can analyze `X_train` and `y_train` as much as we want when designing or optimizing a model. However, you should avoid doing EDA on your testing set (i.e., out-of-sample data) to avoid _data leakage_. Data leakage occurs when information from your out-of-sample data is incorporated into your models (often unintentionally). In general, you perform EDA to guide your modeling decisions, so if your out-of-sample data is included in your EDA then it may influence your modeling decisions."]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"TQJoUyNjIufJ"},"source":["## EDA\n","Use the data frame `df_train` to do all your EDA to ensure that there is no data leakage.\n","\n","We can use the `describe()` method on a data frame to print out a summary of the columns. Use the output of `describe()` or write your own code where appropriate to answer the following questions."]},{"cell_type":"markdown","metadata":{"id":"jFMF3ePiA4sU"},"source":["### Exercises\n","\n","1. Apply the `describe()` method to `df_train`."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"BW4eo_IxIufJ","colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"status":"ok","timestamp":1667225539950,"user_tz":240,"elapsed":211,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"6dacf812-202a-4802-d9c1-b54b54275229"},"source":["# Write your code here.  \n","# -------------------\n","\n","\n","df_train.describe()\n","\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       CreditPolicy      IntRate  Installment          Dti         Fico  \\\n","count   6704.000000  6704.000000  6704.000000  6704.000000  6704.000000   \n","mean       0.801014     0.122590   319.644112    12.609300   710.907369   \n","std        0.399267     0.027047   206.869072     6.882637    38.010970   \n","min        0.000000     0.060000    15.670000     0.000000   612.000000   \n","25%        1.000000     0.103300   164.020000     7.250000   682.000000   \n","50%        1.000000     0.122100   268.530000    12.600000   707.000000   \n","75%        1.000000     0.141100   434.867500    17.940000   737.000000   \n","max        1.000000     0.216400   922.420000    29.950000   827.000000   \n","\n","       DaysWithCrLine      RevolBal    RevolUtil  InqLast6mths   Delinq2yrs  \\\n","count     6704.000000  6.704000e+03  6704.000000   6704.000000  6704.000000   \n","mean      4557.635889  1.694455e+04    46.708816      1.602327     0.159755   \n","std       2501.947628  3.532676e+04    29.055547      2.184938     0.524771   \n","min        180.041667  0.000000e+00     0.000000      0.000000     0.000000   \n","25%       2819.958333  3.152250e+03    22.600000      0.000000     0.000000   \n","50%       4110.041667  8.546500e+03    46.100000      1.000000     0.000000   \n","75%       5730.041667  1.808175e+04    70.700000      2.000000     0.000000   \n","max      17616.000000  1.207359e+06   119.000000     33.000000    11.000000   \n","\n","            PubRec     AnnualInc  Purpose_all_other  Purpose_credit_card  \\\n","count  6704.000000  6.704000e+03        6704.000000          6704.000000   \n","mean      0.062351  6.832708e+04           0.243586             0.131265   \n","std       0.262517  6.229470e+04           0.429278             0.337715   \n","min       0.000000  1.896000e+03           0.000000             0.000000   \n","25%       0.000000  3.840000e+04           0.000000             0.000000   \n","50%       0.000000  5.518800e+04           0.000000             0.000000   \n","75%       0.000000  8.000000e+04           0.000000             0.000000   \n","max       4.000000  2.039784e+06           1.000000             1.000000   \n","\n","       Purpose_debt_consolidation  Purpose_educational  \\\n","count                 6704.000000          6704.000000   \n","mean                     0.410949             0.037739   \n","std                      0.492043             0.190578   \n","min                      0.000000             0.000000   \n","25%                      0.000000             0.000000   \n","50%                      0.000000             0.000000   \n","75%                      1.000000             0.000000   \n","max                      1.000000             1.000000   \n","\n","       Purpose_home_improvement  Purpose_major_purchase  \\\n","count               6704.000000             6704.000000   \n","mean                   0.065185                0.045943   \n","std                    0.246870                0.209377   \n","min                    0.000000                0.000000   \n","25%                    0.000000                0.000000   \n","50%                    0.000000                0.000000   \n","75%                    0.000000                0.000000   \n","max                    1.000000                1.000000   \n","\n","       Purpose_small_business  NotFullyPaid  \n","count             6704.000000   6704.000000  \n","mean                 0.065334      0.157518  \n","std                  0.247133      0.364316  \n","min                  0.000000      0.000000  \n","25%                  0.000000      0.000000  \n","50%                  0.000000      0.000000  \n","75%                  0.000000      0.000000  \n","max                  1.000000      1.000000  "],"text/html":["\n","  <div id=\"df-e2dffaf8-8172-4169-9168-4f526a1ff6b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditPolicy</th>\n","      <th>IntRate</th>\n","      <th>Installment</th>\n","      <th>Dti</th>\n","      <th>Fico</th>\n","      <th>DaysWithCrLine</th>\n","      <th>RevolBal</th>\n","      <th>RevolUtil</th>\n","      <th>InqLast6mths</th>\n","      <th>Delinq2yrs</th>\n","      <th>PubRec</th>\n","      <th>AnnualInc</th>\n","      <th>Purpose_all_other</th>\n","      <th>Purpose_credit_card</th>\n","      <th>Purpose_debt_consolidation</th>\n","      <th>Purpose_educational</th>\n","      <th>Purpose_home_improvement</th>\n","      <th>Purpose_major_purchase</th>\n","      <th>Purpose_small_business</th>\n","      <th>NotFullyPaid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6.704000e+03</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6.704000e+03</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.801014</td>\n","      <td>0.122590</td>\n","      <td>319.644112</td>\n","      <td>12.609300</td>\n","      <td>710.907369</td>\n","      <td>4557.635889</td>\n","      <td>1.694455e+04</td>\n","      <td>46.708816</td>\n","      <td>1.602327</td>\n","      <td>0.159755</td>\n","      <td>0.062351</td>\n","      <td>6.832708e+04</td>\n","      <td>0.243586</td>\n","      <td>0.131265</td>\n","      <td>0.410949</td>\n","      <td>0.037739</td>\n","      <td>0.065185</td>\n","      <td>0.045943</td>\n","      <td>0.065334</td>\n","      <td>0.157518</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.399267</td>\n","      <td>0.027047</td>\n","      <td>206.869072</td>\n","      <td>6.882637</td>\n","      <td>38.010970</td>\n","      <td>2501.947628</td>\n","      <td>3.532676e+04</td>\n","      <td>29.055547</td>\n","      <td>2.184938</td>\n","      <td>0.524771</td>\n","      <td>0.262517</td>\n","      <td>6.229470e+04</td>\n","      <td>0.429278</td>\n","      <td>0.337715</td>\n","      <td>0.492043</td>\n","      <td>0.190578</td>\n","      <td>0.246870</td>\n","      <td>0.209377</td>\n","      <td>0.247133</td>\n","      <td>0.364316</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.060000</td>\n","      <td>15.670000</td>\n","      <td>0.000000</td>\n","      <td>612.000000</td>\n","      <td>180.041667</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.896000e+03</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.000000</td>\n","      <td>0.103300</td>\n","      <td>164.020000</td>\n","      <td>7.250000</td>\n","      <td>682.000000</td>\n","      <td>2819.958333</td>\n","      <td>3.152250e+03</td>\n","      <td>22.600000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.840000e+04</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.000000</td>\n","      <td>0.122100</td>\n","      <td>268.530000</td>\n","      <td>12.600000</td>\n","      <td>707.000000</td>\n","      <td>4110.041667</td>\n","      <td>8.546500e+03</td>\n","      <td>46.100000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>5.518800e+04</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.000000</td>\n","      <td>0.141100</td>\n","      <td>434.867500</td>\n","      <td>17.940000</td>\n","      <td>737.000000</td>\n","      <td>5730.041667</td>\n","      <td>1.808175e+04</td>\n","      <td>70.700000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>8.000000e+04</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>0.216400</td>\n","      <td>922.420000</td>\n","      <td>29.950000</td>\n","      <td>827.000000</td>\n","      <td>17616.000000</td>\n","      <td>1.207359e+06</td>\n","      <td>119.000000</td>\n","      <td>33.000000</td>\n","      <td>11.000000</td>\n","      <td>4.000000</td>\n","      <td>2.039784e+06</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2dffaf8-8172-4169-9168-4f526a1ff6b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e2dffaf8-8172-4169-9168-4f526a1ff6b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e2dffaf8-8172-4169-9168-4f526a1ff6b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"Qh2ClVg8IufM"},"source":["2. What proportion of borrows default on their loans (i.e., did not fully pay back loan)?"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"iY8GkdalIufN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667225539951,"user_tz":240,"elapsed":12,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"df323011-61b8-4867-8fbb-e83b11c6d9e1"},"source":["# Write your code here.  \n","# -------------------\n","df_train.NotFullyPaid.mean()\n","\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1575178997613365"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"6ZJcH9gyIufQ"},"source":["3. Which of the purposes for borrowing is most likely to lead to a default?"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ETdlFvRJIufQ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1667225539951,"user_tz":240,"elapsed":8,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"dc4f8564-fc48-472e-e74f-34c3b10c9787"},"source":["# Write your code here.  \n","# -------------------\n","\n","\n","df_summary = df_train.groupby('NotFullyPaid').sum().loc[:,'Purpose_all_other':'Purpose_small_business']\n","(df_summary.iloc[1]/ (df_summary.iloc[0]+df_summary.iloc[1])).idxmax()\n","\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Purpose_small_business'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"RA9AXQbdIufT"},"source":["4. Which of the purposes has the highest correlation with the `NotFullyPaid` variable?\n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"KhJFB-8DIufT","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1667225539951,"user_tz":240,"elapsed":7,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"1088d5cd-8ef0-4e8a-d43b-308f25ba6bc5"},"source":["# Write your code here.  \n","# -------------------\n","\n","df_train.corr().NotFullyPaid.drop('NotFullyPaid').abs().idxmax()\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CreditPolicy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"jkFC29XhIufY"},"source":["# Tracking progress\n","\n","Throughout this lab we make several models. In this section we will initialize a function `make_models` and a data frame `all_models` that we will use throughout the lab. "]},{"cell_type":"markdown","metadata":{"id":"rkuHhysLEXcN"},"source":["## Defining the `make_models` function\n","\n","We will apply the seven techniques in this lab using four different models, which are initialized in the `make_models` function below.\n","\n","* _LR_L2_: is a logistic regression with an L2 loss\n","* _LR_L1_: is a logistic regression with an L1 loss with \"balanced\" class weights\n","* _CART_: is a CART tree with \"balanced\" class weights\n","* _RF_: is a random forest with \"balanced\" class weights\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AlT2vkAyEXs9"},"source":["def make_models():\n","  \"\"\"Makes a dictionary of four untrained models\"\"\"\n","  \n","  return {\n","      'LR_L2': LogisticRegression(random_state=0, max_iter=200),\n","      'LR_L1': LogisticRegression(random_state=0, penalty='l1', solver='liblinear', class_weight='balanced', max_iter=500),\n","      'CART': DecisionTreeClassifier(random_state=0, class_weight='balanced'),\n","      'RF': RandomForestClassifier(random_state=0, class_weight='balanced'),\n","  }\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjsTfAlPMHpT"},"source":["### Exercise\n","\n","> 1. Notice that we set the parameter `class_weights` to `'balanced'`. Look up the documentation for `LogisticRegression`. What does the `class_weights = 'balanced'` do? Why do you think its appropriate here?\n"]},{"cell_type":"markdown","metadata":{"id":"T68pTHazEYks"},"source":["## Initializing the `all_models` data frame\n","\n","In the code block below we initialize the `all_models` data frame. You don't need to understand every line of code, the more important this is to understand that we have a data frame called `all_models` that we will use throughout this lab to track progress."]},{"cell_type":"code","metadata":{"id":"zYdNb-vGxAE7","colab":{"base_uri":"https://localhost:8080/","height":959},"executionInfo":{"status":"ok","timestamp":1667225540436,"user_tz":240,"elapsed":9,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"6d330d3a-977a-4960-c25c-1e6385b685d4"},"source":["'''Create a data frame to keep track of all the models we train in this lab'''\n","\n","# Initialize a tuple of names for each model\n","model_names = ('LR_L2',  # Logistic regression with L2 regularizer\n","               'LR_L1',  # Logistic regression with L1 regularizer\n","               'CART',  # CART tree classifier\n","               'RF'  # Random forest classifier \n","               )\n","# Initialize a tuple of technique names that we will cover\n","engineering_techniques = ('Baseline',  # Set of baseline models\n","                          'Scaling',  # Set of models trained with scaled data\n","                          'Feature Engineering',  # Set of models trained with engineered features\n","                          'Feature_Selection',  # Set of models trained with \"selected\" features\n","                          'Grid Search',  # Set of models trained via grid search\n","                          'Stacking',  # Set of stacked model \n","                          'Bagging'  # A bagged model\n","                          )\n","\n","# Initialize the multi indices of the `all_models` data frame\n","df_indices = pd.MultiIndex.from_product([model_names, engineering_techniques], names=('model names', 'technique'))\n","# Initialize the `all_models` data frame\n","all_models = pd.DataFrame(index=df_indices, columns=['Precision', 'Recall', 'Score', 'Model'])\n","all_models[['Precision', 'Recall', 'Score']] = all_models[['Precision', 'Recall', 'Score']].astype(float)\n","all_models  # Initialized data frame only has NaNs, which is perfect!"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 Precision  Recall  Score Model\n","model names technique                                          \n","LR_L2       Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN\n","LR_L1       Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN\n","CART        Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN\n","RF          Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN"],"text/html":["\n","  <div id=\"df-7c503c48-ff82-4769-8dbc-22a723327420\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th>technique</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">LR_L2</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">LR_L1</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">CART</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">RF</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c503c48-ff82-4769-8dbc-22a723327420')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7c503c48-ff82-4769-8dbc-22a723327420 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7c503c48-ff82-4769-8dbc-22a723327420');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"KAw6taJkvu1N"},"source":["We're initialized a data frame `all_models`. Over the course of this lab, we will replace the NaNs in this data frame. "]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"UHqH0ApjIufb"},"source":["# Model selection (Baseline)\n","\n","For starters, we should fit our four baseline models. We will score each model using its average precision and recall (i.e., $\\frac{precision + recall}{2}$). We covered precision and recall in lab two, but you can refer to [this article](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) for a refresher."]},{"cell_type":"markdown","metadata":{"id":"a-L_dM7TH-p1"},"source":["### Exercise\n","\n","> 1. Complete the function below, which fits multiple models on a training dataset, and evaluates its precision, recall, and the average of precision and recall on the out-of-sample data. You should use the `precision_score()` and `recall_score()` functions below to calculate those metrics."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"YL8LwfNAIufb","colab":{"base_uri":"https://localhost:8080/","height":330},"executionInfo":{"status":"ok","timestamp":1667225546986,"user_tz":240,"elapsed":6558,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"679d638f-f148-4f76-b3d4-40e631c4e694"},"source":["# from sklearn.metrics import precision_score, recall_score\n","\n","def fit_and_score_model(all_models, stage_name, X_train, X_out_of_sample, y_train, y_out_of_sample):\n","    \"\"\"Fits the models that are initialized by models_dict on the X_train and y_train\n","    data, and evaluates the model on the out-of-sample data X_out_of_sample and y_out_of_sample\"\"\"\n","    \n","    # Make a dictionary of models\n","    models_dict = make_models()\n","\n","    # Loop through each model in model_dict\n","    for model_name in models_dict:\n","        model = models_dict[model_name]\n","        \n","        '''Write some code to fit the model, and calculate precision (call it \n","        model_precision), recall (call it model_recall), and score (call it \n","        model_score) on the testing set.'''\n","\n","        # Write your code here.  \n","        # -------------------------------------------------------------------------\n","        \n","        model.fit(X_train, y_train)\n","\n","        model_precision = precision_score(y_out_of_sample, model.predict(X_out_of_sample))\n","        model_recall = recall_score(y_out_of_sample, model.predict(X_out_of_sample))\n","        model_score = (model_precision+model_recall)/2\n","\n","        \n","        # -------------------------------------------------------------------------\n","        print(f'{model_name} achieved a precision of {model_precision:.3f} and recall of {model_recall:.3f}')\n","        \n","        all_models.loc[model_name, stage_name] = np.array((model_precision, model_recall, model_score, model), dtype='object')\n","\n","    return all_models\n","    \n","all_models = fit_and_score_model(all_models, 'Baseline', X_train, X_test, y_train, y_test)\n","all_models.loc[:, 'Baseline', :].head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["LR_L2 achieved a precision of 0.000 and recall of 0.000\n","LR_L1 achieved a precision of 0.257 and recall of 0.579\n","CART achieved a precision of 0.192 and recall of 0.191\n","RF achieved a precision of 0.429 and recall of 0.006\n"]},{"output_type":"execute_result","data":{"text/plain":["                       Precision    Recall     Score  \\\n","model names technique                                  \n","LR_L2       Baseline    0.000000  0.000000  0.000000   \n","LR_L1       Baseline    0.256506  0.578616  0.417561   \n","CART        Baseline    0.191579  0.190776  0.191177   \n","RF          Baseline    0.428571  0.006289  0.217430   \n","\n","                                                                   Model  \n","model names technique                                                     \n","LR_L2       Baseline    LogisticRegression(max_iter=200, random_state=0)  \n","LR_L1       Baseline   LogisticRegression(class_weight='balanced', ma...  \n","CART        Baseline   DecisionTreeClassifier(class_weight='balanced'...  \n","RF          Baseline   (DecisionTreeClassifier(max_features='auto', r...  "],"text/html":["\n","  <div id=\"df-a5cc268a-e660-458b-b335-7aad63e45fd9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th>technique</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>LR_L2</th>\n","      <th>Baseline</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th>LR_L1</th>\n","      <th>Baseline</th>\n","      <td>0.256506</td>\n","      <td>0.578616</td>\n","      <td>0.417561</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>CART</th>\n","      <th>Baseline</th>\n","      <td>0.191579</td>\n","      <td>0.190776</td>\n","      <td>0.191177</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>RF</th>\n","      <th>Baseline</th>\n","      <td>0.428571</td>\n","      <td>0.006289</td>\n","      <td>0.217430</td>\n","      <td>(DecisionTreeClassifier(max_features='auto', r...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5cc268a-e660-458b-b335-7aad63e45fd9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a5cc268a-e660-458b-b335-7aad63e45fd9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a5cc268a-e660-458b-b335-7aad63e45fd9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"cvjWh3sHT9HK"},"source":["Upon examining the `all_models` data frame, you should see that it keep tracks of the precision, recall, and a score (float types) in addition to the trained models (object type). You should also see that the baseline \"LR_L2\" model is very bad, but the other models looks a little more promising. "]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"4WXcJxNZIufe"},"source":["### Exercises\n","\n","> 1. We provided some code that will use the baseline \"LR_L2\" model to predict target values. Describe those predictions. Does anything seem strange? (**Hint**: you should see an `UndefinedMetricWarning` after running the previous code block that mentions 'zero\\_division'.)"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"LRzsFIjDIufe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667225546986,"user_tz":240,"elapsed":8,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"96def387-857c-4b6d-a5c4-5a48d3c51576"},"source":["# Predict the target values using the logistic regression model\n","y_test_predictions = all_models.Model.loc['LR_L2','Baseline'].predict(X_test)\n","\n","# Write your code here\n","# -------------------------------------------------------------------------\n","\n","\n","y_test_predictions.max()\n","\n","# -------------------------------------------------------------------------"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"y_mWwqWIIufh"},"source":["> 2. Why should we use precision and recall as our metrics for this problem rather than the usual ROC metrics?\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"nJskyDxcIufh"},"source":["# Feature scaling\n","Next, lets scale the features to have a mean of zero and standard deviation of one (i.e., standardize). This is a very important step when features have different orders of magnitude. However, when features values have similar orders of magnitude (like they do now) you may see little to no improvement. The easiest way to standardize data is with the `StandardScaler` function, which is \"fit\" (i.e., calculates the mean and standard deviation) on the training set. Once the `StandardScaler` is \"fit\", you should apply it to both the training and testing set before fitting and scoring your prediction model. Note, that we can generally keep binary data ''as is''. Since our target data (whether a loan was repaid) is binary, we won't scale the target, which makes the data scaling step a little easier. However, if the target data was a continuous variable, then we could follow the same steps that we will go through with our feature data to scale the target data."]},{"cell_type":"markdown","metadata":{"id":"JwK1RuQ7Mzkb"},"source":["### Exercise\n","\n","> 1. Complete the function `standardize_data` that initializes a `StandardScaler`, fits it on a training dataset (`X_train`). Use the fit `scaler` to standardize (i.e., transform) the training and out-of-sample data (`X_test`), and call the variables `X_train_standardized` and `X_out_of_sample_standardized`, respectively.)"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"bvHvuBsrIufi","colab":{"base_uri":"https://localhost:8080/","height":276},"executionInfo":{"status":"ok","timestamp":1667226221920,"user_tz":240,"elapsed":2509,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"09c9d488-399e-4cd1-d802-437e03a1bfb6"},"source":["# from sklearn.preprocessing import StandardScaler\n","\n","technique_name = 'Scaling'\n","\n","def standardize_data(X_train, X_out_of_sample):\n","    \"\"\"standardizes all of the data in X_train and X_out_of_sample. The mean and\n","    standard deviation of each feature (i.e., each column) from the X_train\n","    data is used to standardize both the X_train and X_out_of sample.\"\"\"\n","\n","    # Initialize data frame for scaled data\n","    X_train_standardized = X_train.copy()\n","    X_out_of_sample_standardized = X_out_of_sample.copy()\n","\n","    # Define scaling function\n","    scaler = StandardScaler()\n","    \n","    '''Use scaler to standardize your data. You'll need to fit scaler with your\n","    training data (use the fit method) and standardize your training and \n","    out-of-sample data (use the transform method)'''\n","    # -------------------------------------------------------------------------\n","\n","    scaler.fit(X_train)\n","    X_train_standardized[:] = scaler.transform(X_train)\n","    X_out_of_sample_standardized[:] = scaler.transform(X_out_of_sample)\n","    \n","\n","    \n","                        \n","    # -------------------------------------------------------------------------\n","\n","    return X_train_standardized, X_out_of_sample_standardized, scaler\n","\n","# Make new data that is scaled\"\n","X_train_scaled, X_test_scaled, scaler = standardize_data(X_train, X_test)\n","\n","# Fit and score a model trained with scaled data\n","all_models = fit_and_score_model(all_models, technique_name, X_train_scaled, X_test_scaled, y_train, y_test)\n","all_models.loc[:, technique_name, :].head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LR_L2 achieved a precision of 0.471 and recall of 0.017\n","LR_L1 achieved a precision of 0.257 and recall of 0.587\n","CART achieved a precision of 0.192 and recall of 0.191\n","RF achieved a precision of 0.333 and recall of 0.004\n"]},{"output_type":"execute_result","data":{"text/plain":["                       Precision    Recall     Score  \\\n","model names technique                                  \n","LR_L2       Scaling     0.470588  0.016771  0.243680   \n","LR_L1       Scaling     0.257353  0.587002  0.422178   \n","CART        Scaling     0.191579  0.190776  0.191177   \n","RF          Scaling     0.333333  0.004193  0.168763   \n","\n","                                                                   Model  \n","model names technique                                                     \n","LR_L2       Scaling     LogisticRegression(max_iter=200, random_state=0)  \n","LR_L1       Scaling    LogisticRegression(class_weight='balanced', ma...  \n","CART        Scaling    DecisionTreeClassifier(class_weight='balanced'...  \n","RF          Scaling    (DecisionTreeClassifier(max_features='auto', r...  "],"text/html":["\n","  <div id=\"df-fe6cf411-f654-4410-9476-be01f634d157\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th>technique</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>LR_L2</th>\n","      <th>Scaling</th>\n","      <td>0.470588</td>\n","      <td>0.016771</td>\n","      <td>0.243680</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th>LR_L1</th>\n","      <th>Scaling</th>\n","      <td>0.257353</td>\n","      <td>0.587002</td>\n","      <td>0.422178</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>CART</th>\n","      <th>Scaling</th>\n","      <td>0.191579</td>\n","      <td>0.190776</td>\n","      <td>0.191177</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>RF</th>\n","      <th>Scaling</th>\n","      <td>0.333333</td>\n","      <td>0.004193</td>\n","      <td>0.168763</td>\n","      <td>(DecisionTreeClassifier(max_features='auto', r...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe6cf411-f654-4410-9476-be01f634d157')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fe6cf411-f654-4410-9476-be01f634d157 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fe6cf411-f654-4410-9476-be01f634d157');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"RoSM6ayT3Dxe"},"source":["## Defining the compare_models function\n","\n","Let's define the function `compare_models` to compare the models based on the technique `technique_name` to the \"Baseline\" models"]},{"cell_type":"code","metadata":{"id":"n0fJt13O3ED6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667226224034,"user_tz":240,"elapsed":151,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"b4f65bfb-6d90-42a5-8daf-24a4bf93d9cd"},"source":["def compare_models(technique_name):\n","  \"\"\"Prints out the average and biggest improvement observed between the \n","  models trained with technique_name and the Baseline models\"\"\"\n","\n","  # Evaluate score differences\n","  score_differences = (all_models.loc[:, technique_name, :].Score.values - all_models.loc[:, 'Baseline', :].Score.values)\n","  \n","  # Get the average and biggest score improvement\n","  mean_score_difference = score_differences.mean()\n","  most_score_improvement = score_differences.max()\n","\n","  print(f'On average, scores improved by {mean_score_difference:.3f}, and the most improvement was {most_score_improvement:.3f}')\n","\n","technique_name = 'Scaling'\n","compare_models(technique_name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On average, scores improved by 0.050, and the most improvement was 0.244\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"q-BFujZNIufk"},"source":["## Using a fit standardization method\n","If you want to convert your features (or target variables if they were scaled) back into the units from the original features, then you can use the `inverse_transform` method as we do below. Note, that the mean of the transformed training set should always be zero, but the mean of the out-of-sample set will likely just be close to zero because the standardization is based on the training set mean and standard deviation (using the testing set is cheating!). Below is just an example of how to run a quick sanity check to ensure the scaling is working properly."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"gcnjHmhQIufk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667226226721,"user_tz":240,"elapsed":138,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"5de2b12e-742a-47e8-b05e-7a8b20b81dee"},"source":["# Transform the data back to original units\n","transformed_X_train = scaler.inverse_transform(X_train_scaled)\n","transformed_X_test = scaler.inverse_transform(X_test_scaled)\n","\n","# Evaluate the average of each data set\n","print(f'The mean of the scaled feature in column 0 is {X_train_scaled.iloc[:, 0].mean():.3f} and {X_test_scaled.iloc[:, 0].mean():.3f} for training and testing, respectively.')\n","print(f'The mean of the un-scaled feature in column 0 is {transformed_X_train[:, 0].mean():.3f} and {transformed_X_test[:,0].mean():.3f} for training and testing, respectively.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The mean of the scaled feature in column 0 is -0.000 and 0.033 for training and testing, respectively.\n","The mean of the un-scaled feature in column 0 is 0.801 and 0.814 for training and testing, respectively.\n"]}]},{"cell_type":"markdown","metadata":{"id":"wskfl5J1X6IV"},"source":["### Exercises\n","\n","> 1. You should see that the mean of the first feature in `X_train_scaled` is 0, but its mean in `X_test_scaled` is  0.033. Why isn't the mean of `X_test_scaled` 0 after we've standardized it?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"CFXNd_jBIufm"},"source":["# Feature Engineering\n","\n","In this section, we will see that our dataset has many features that are not useful for predicting the target. When this is the case, you should try to collect better data, however, when better data in unavailable we must do the best with what we have. One option is to create new, strong features that are more useful for predicting the target by transforming and combining weak features (like we did in Lab 1!). This practice is known as feature engineering.\n","\n","Generally, good features have several desirable characteristics, for example, they:\n","*   are spread out evenly (e.g., Gaussian or uniform distributions)\n","*   behave differently (e.g., they may have different means) when the target is different\n","*   are normalized to similar scales.\n","\n","One common technique is to apply logarithms or square roots to features with very large values that follow a distribution with a long tail (e.g., population, income). The problem with distributions that have large values and long tails is that the absolute change in the value of a feature gives less information than the relative change. For example, a 100 dollar difference in income means very different things if you are comparing two low income data points versus two high income data points.\n","\n","We can also \"combine\" features (e.g., add them, multiply them) to make them more useful. When we combine features we can take advantage of *interaction effects* (a concept from statistics), where the interaction of two features together is greater than the sum of the parts. For example, if two features correlate strongly with a target, it is likely that the product of the two features will also correlate. \n","\n","Constructing good features requires experience, domain expertise, and some luck. If you have domain knowledge about your application then you should  think about whether a transformation or a new feature makes sense for the problem, and you should always check that your engineered features are useful for predicting a target. You can generally do anything to create new features. The common tricks include multiplying and/or dividing multiple existing features, however, there are two major rules related to feature engineering:\n","\n","1. Do not create too many features, especially in order to chase marginal increases in correlation. If the correlation is small and your dataset is small, you might be creating fake (dataset-dependent) correlations that won't generalize outside of your training data.\n","2. You **cannot** use the target variable to create a feature."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Euay2rOoIufm"},"source":["## Transforming your data to get better features\n","\n","In this lab, we will only do feature engineering by exploiting interaction effects between multiple features. The best way to go about this is by domain knowledge and developed intuition, however, there are also some tricks to make features without domain knowledge, which will be the focus of this section. \n","\n","An algorithmic way to generate features is provided in the code block below. Where we loop through every pair of features, and multiply them features together (i.e., \"combine\" them). We then evaluate the _usefulness_ of the combined features. There are many metrics for feature selection ([see list of `sklearn` functions here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)). We will use the function `f_classif` to evaluate the  **An**alysis **o**f **Va**riance (ANOVA) F-Value, which is appropriate for estimating how useful a feature is for predicting a categorical target. A higher F-value suggests the feature will be better, and a $p$-value $<$ 0.05 suggests the F-value returned is based on a real trends in the data (i.e., not random chance).\n","\n","For more detail on the ANOVA F-Value you can refer to[this article](https://machinelearningmastery.com/feature-selection-with-numerical-input-data/#:~:text=ANOVA%20f%2Dtest%20Feature%20Selection,-ANOVA%20is%20an&text=The%20ANOVA%20method%20is%20a,as%20an%20ANOVA%20f%2Dtest.&text=The%20results%20of%20this%20test,be%20removed%20from%20the%20dataset). However, please keep in mind that `f_classif` should only be used when the target is categorical. As a result, you should use other tests if the target is continuous.\n","\n","**Note:** The code block below uses a 'magic command' ([more info here](https://ipython.readthedocs.io/en/stable/interactive/magics.html?highlight=capture#cellmagic-capture )) to suppress warning messages that would normally come up when running `f_classif` in this example. Those warnings can be safely ignored, and this magic command provides and easy way to hide them :)  \n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"exOOv6dNIufn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667228930449,"user_tz":240,"elapsed":735,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"3cec9cee-c8ad-4a6e-c0d2-b45f466e0433"},"source":["%%capture --no-stdout  \n","# from sklearn.feature_selection import f_classif\n","# Create a list of F-values for the existing features\n","feature_F_scores, _ = f_classif(X_train, y_train)\n","\n","# Iterate through each combination of features\n","for f1_index, f1 in enumerate(X_train.columns):\n","  for f2_index, f2 in enumerate(X_train.columns[f1_index + 1:]):\n","    # Multiply the two features to create a new feature\n","    new_feature = X_train[[f1]].multiply(X_train[f2], axis=0)\n","    # Evaluate F-value of new feature\n","    F_Score_new, p_value_new = f_classif(new_feature, y_train)\n","    # Evaluate the relative improvement of the new feature\n","    F_score_improvement = F_Score_new[0] / max(feature_F_scores[[f1_index, f2_index]])\n","    # Print out features that is sufficiently improved \n","    if F_score_improvement >= 1.5 and F_Score_new[0] >= 75 and p_value_new < 0.05:\n","        '''Note that F_score_improvement >= 1.5 and F_Score_new[0] >= 75 is\n","         relatively arbitrary, and that other values could be used.'''\n","        print(f'{f1} + {f2} has an F-score of {F_Score_new[0]:.2f}')\n","        print(f'\\tBetter by a factor of {F_score_improvement:.2f} over features in isolation')\n","        print(f'\\tThe result is significant (p = {p_value_new})')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installment + InqLast6mths has an F-score of 134.11\n","\tBetter by a factor of 8.57 over features in isolation\n","\tThe result is significant (p = [1.01107633e-30])\n","DaysWithCrLine + InqLast6mths has an F-score of 96.81\n","\tBetter by a factor of 6.18 over features in isolation\n","\tThe result is significant (p = [1.08827276e-22])\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"RyYDpyD-Iufp"},"source":["### Exercise\n","\n","> 1. Play around with the code above to see if you can find any new interesting or useful features. For example, you could try different transforms of the features, or combining three features together. Oftentimes, this task of feature engineering is the most important and time-consuming aspect of machine learning. Good features are hard to come by!\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"5Ou2HijfpkvO"},"source":["## Using transformed features to train a model\n","\n","It is important to not get too carried away with algorithmic feature engineering. If we construct a new feature with domain knowledge, we should think carefully about why it is relevant. If you have a small amount of data, you don't want to create idiosyncrasies in your dataset that don't reflect real life. The danger with feature engineering, is that you may create features that guide your model towards patterns that don't actually exist (i.e., overfitting).\n","\n","### Exercise\n","> 1. In the code below, we've proposed a few additional useful features and created a function that adds them to the dataset (based on the previous code block). Feel free to add any additional features, or change the existing ones, and try to improve the precision and recall."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"fbInOhsdIufq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667229199101,"user_tz":240,"elapsed":5596,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"25021e68-e4b9-4805-8222-80a0daf7d8e0"},"source":["technique_name = 'Feature Engineering'\n","\n","def new_feature_combos(X):\n","    \"\"\"\n","    Make a new set of X data by doing algorithmic feature engineering\n","    \"\"\"\n","\n","    # First, we make a copy of the original X features\n","    X_new = X.copy()\n","    \n","    # Then we add new features\n","    X_new['Installment * InqLast6mths'] = X_new.Installment * X_new.InqLast6mths\n","    X_new['DaysWithCrLine * InqLast6mths'] = X_new.DaysWithCrLine * X_new.InqLast6mths\n","\n","    # Add any additional features that you might have found here\n","    # ----------------------------------------------------------\n","    \n","    \n","    \n","    # ----------------------------------------------------------\n","    return X_new\n","\n","# Make new X features with interactions\n","X_train_interactions = new_feature_combos(X_train)\n","X_test_interactions = new_feature_combos(X_test)\n","\n","# Fit and score the model, save it to the all models dictionary\n","all_models = fit_and_score_model(all_models, technique_name, X_train_interactions, X_test_interactions, y_train, y_test)\n","compare_models(technique_name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LR_L2 achieved a precision of 0.692 and recall of 0.019\n","LR_L1 achieved a precision of 0.259 and recall of 0.591\n","CART achieved a precision of 0.191 and recall of 0.176\n","RF achieved a precision of 0.375 and recall of 0.006\n","On average, scores improved by 0.082, and the most improvement was 0.356\n"]}]},{"cell_type":"markdown","metadata":{"id":"wSUooLYFp0IC"},"source":["# Feature selection\n","\n","Often times we have a lot of features that we don't actually need and keeping those extra features (i.e., features that are not useful for predicting a target) can reduce model performance. We can use feature section to pick the best set of features for our model and address this issue."]},{"cell_type":"markdown","metadata":{"id":"4NQ_wA00pJAv"},"source":["## Doing feature selection\n","\n","As in the previous section, we will use F-values to evaluate how _useful_ a feature is for predicting a target. We will also introduce a new function `SelectKBest` from `sklearn` that picks the `k` best features, where \"best\" is quantified by a function (e.g., `f_classif`). We will use `f_classif` in this lab because default prediction is a classification problem. For regression problems, we should use `f_regression`."]},{"cell_type":"markdown","metadata":{"id":"oRr6lEY0uyNE"},"source":["### Exercises\n","\n","> 1. Use function `SelectKBest` to select the ten most important features, and report their names.\n"]},{"cell_type":"code","metadata":{"id":"VwdftBqsvAF6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667230290377,"user_tz":240,"elapsed":5,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"6c14d6d4-22bb-4b3e-8218-307c5f791362"},"source":["# Write your code here\n","# ----------------------------------------------------------\n","\n","\n","select_features = SelectKBest(f_classif, k=10)\n","select_features = select_features.fit(X_train, y_train)\n","feature_mask = select_features.get_support()\n","X_train.columns[feature_mask]\n","\n","# ----------------------------------------------------------\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['CreditPolicy', 'IntRate', 'Installment', 'Fico', 'RevolBal',\n","       'RevolUtil', 'InqLast6mths', 'PubRec', 'Purpose_credit_card',\n","       'Purpose_small_business'],\n","      dtype='object')"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"-OAKJTGFvAdH"},"source":["> 2. Compare the models using these ten features against the baseline models"]},{"cell_type":"code","metadata":{"id":"4VeL_YfMuyXu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667230836300,"user_tz":240,"elapsed":3291,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"c94291ec-8d59-4589-8f72-8088bb76bd0a"},"source":["# Write your code here\n","# ----------------------------------------------------------\n","\n","technique_name = 'Feature_Selection'\n","\n","def new_feature_selections(X_train, y_train, X_test, y_test):\n","\n","    select_features = SelectKBest(f_classif, k=10)\n","    select_features = select_features.fit(X_train, y_train)\n","    feature_mask = select_features.get_support()\n","\n","    X_train_feature_selection = X_train.iloc[:, feature_mask]\n","    X_test_feature_selection = X_test.iloc[:, feature_mask]\n","\n","    return X_train_feature_selection, y_train, X_test_feature_selection, y_test\n","\n","# Make new X features with interactions\n","X_train_feature_selection, y_train, X_test_feature_selection, y_test = new_feature_selections(X_train, y_train, X_test, y_test)\n","\n","# Fit and score the model, save it to the all models dictionary\n","all_models = fit_and_score_model(all_models, technique_name, X_train_feature_selection, X_test_feature_selection, y_train, y_test)\n","compare_models(technique_name)\n","\n","# ----------------------------------------------------------"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LR_L2 achieved a precision of 0.733 and recall of 0.023\n","LR_L1 achieved a precision of 0.259 and recall of 0.572\n","CART achieved a precision of 0.224 and recall of 0.214\n","RF achieved a precision of 0.389 and recall of 0.015\n","On average, scores improved by 0.097, and the most improvement was 0.378\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"vjgS2d6pORdu"},"source":["# Grid search\n","\n","Choosing hyperparameters for a machine learning model is non-trivial, and many of the machine learning models that we use have a wide variety of hyperparameters. For example, logistic regression on scikit-learn allows 'L1' or 'L2' regularization, a regularization weight 'C', and additional modifications to the loss function (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for full list). Decision trees have parameters for depth, loss function, and splitting rules (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for full list). \n","\n","When we tune a single parameter, the common method is to use k-fold cross-validation and evaluate for which setting does the model perform the best. However, how do we tune 5 or 6 different parameters all at once? The standard method is known as _grid-searching_, where we simply do k-fold cross-validation on all possible combinations of the different parameter values. In grid searching, the practice is to define a specific scoring function, perform a search, and identify an appropriate model. Of course, this approach is computationally expensive, especially when there is a large number of parameters or values to consider. Interestingly, researchers have found that randomly searching the parameter space is often just as effective as grid searching. What this suggests is that there is often no good intuitive way to choose hyperparameters.\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"JoE9YUcoORdu"},"source":["## Parameter tuning via grid search\n","\n","Scikit-learn provides two useful functions `GridSearchCV()` and `RandomSearchCV()` to implement two approaches towards selecting the hyperparameters. These functions take in a model, a dictionary of parameters over which to sweep, and a scoring function to evaluate the different models on. For each parameter combination, we will run the function with 5-fold cross-validation to evaluate each of the scoring functions. Scikit-learn provides a [number of predefined scoring functions](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter), but you can also make your own.\n","\n","In this section we will do a grid search to find better hyperparameters of the `DecisionTreeClassifier()`. We will search over four sets of hyperparameters listed below with each combination of arguments listed in the brackets:\n","* `criterion` ('gini' or 'entropy'): sets the loss function on which to make splits.\n","* `min_samples_leaf` (1, 2, 5, 10, or 20): the minimum number of samples for a node to be a leaf\n","* `max_features` ('auto', 'log2', None):  the maximum number of features to consider when searching for splits \n","* `class_weight` ('balanced', None): balances the penalty for misclassified 1's versus 0's"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"5xykbyn2ORdv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667230921654,"user_tz":240,"elapsed":15013,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"15422ed8-33ce-43e2-bc2c-201a49d439e6"},"source":["\"\"\"Remember this is training a CART tree for every combination of parameters\n","in the list params_to_search... this might take 10 to 20 seconds to run :)\"\"\"\n","\n","# Dictionary of parameters to search\n","params_to_search = {\n","    'criterion': ['gini', 'entropy'],\n","    'min_samples_leaf': [1, 2, 5, 10, 20],\n","    'max_features': ['auto', 'log2', None],\n","    'class_weight': ['balanced', None],\n","}\n","\n","# Initialize a model\n","mdl = DecisionTreeClassifier(random_state=0)\n","# Initialize the grid search\n","optimized_dt = GridSearchCV(mdl, params_to_search, scoring = ['recall', 'precision'], refit=False, cv=5)\n","# Run the grid search\n","optimized_dt.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=0),\n","             param_grid={'class_weight': ['balanced', None],\n","                         'criterion': ['gini', 'entropy'],\n","                         'max_features': ['auto', 'log2', None],\n","                         'min_samples_leaf': [1, 2, 5, 10, 20]},\n","             refit=False, scoring=['recall', 'precision'])"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"fYmC9I3QORdx"},"source":["The results from grid searching are all stored in `optimized_dt.cv_results_`.\n","\n","### Exercise\n","> 1. Take a look at the keys and values of `optimized_dt.cv_results_`, and make a scatter plot of the `mean_test_precision` versus `mean_test_recall`."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"qsIQsKjBORdx","colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"status":"ok","timestamp":1667231301909,"user_tz":240,"elapsed":425,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"f6aba85c-865b-4e69-f037-af6ac7cabe53"},"source":["\"Get the mean test precision and precision from optimized_dt.cv_results_and plot a scatter plot.\"\n","\n","# Write your code here\n","# ---------------------------------------------------------\n","\n","mean_test_precision = optimized_dt.cv_results_['mean_test_precision']\n","mean_test_recall = optimized_dt.cv_results_['mean_test_recall']\n","\n","plt.scatter(mean_test_precision, mean_test_recall)\n","plt.xlabel('Precision')\n","plt.ylabel('Recall')\n","\n","# ---------------------------------------------------------"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Recall')"]},"metadata":{},"execution_count":41},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"kiVSFzbnORdz"},"source":["After the grid search you should see that the CART tree performs nearly as well as the best LR_L1 model. However, it seems that if you select the parameters appropriately, you will either swing to high recall or high precision.\n","\n","You can go through the parameters (`params`) along with their corresponding precision (`mean_test_precision`) and recall scores (`mean_test_recall`) inside `cv_results_`. Explore the grid search results and answer the following questions.\n","\n","\n","1. Let's suppose that the \"best\" model is one that maximizes the sum of mean test recall and mean test precision. What parameters does the \"best\" model have based on the results from the grid search?\n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"HP3Wd8xeORdz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667231475459,"user_tz":240,"elapsed":125,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"22c0ed97-a39e-41d1-ec68-bd91fade38ce"},"source":["# Write your code here\n","# -------------------\n","\n","max_score = 0\n","best_model = None\n","\n","params = optimized_dt.cv_results_['params']\n","for par, rec, pre in zip(params, mean_test_recall, mean_test_precision):\n","  if rec + pre > max_score:\n","    best_model = par\n","    max_score = rec + pre\n","\n","print('The best model has a score of: ', max_score)\n","print('The best model has the parameters: ', best_model)\n","\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The best model has a score of:  0.7281296479371463\n","The best model has the parameters:  {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 20}\n"]}]},{"cell_type":"markdown","metadata":{"id":"5q5cgvEmHqA0"},"source":["2. Determine the set of parameters that achieved the highest precision, and the other set that achieved the highest recall. Do the settings make sense intuitively?"]},{"cell_type":"code","metadata":{"id":"xsi3RoQTHqM0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667231856370,"user_tz":240,"elapsed":262,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"45f07403-997e-4c6b-aa13-639fe51a9391"},"source":["# Write your code here\n","# -------------------\n","max_precision = 0\n","best_model_precision = None\n","\n","max_recall = 0\n","best_model_recall = None\n","\n","params = optimized_dt.cv_results_['params']\n","for par, rec, pre in zip(params, mean_test_recall, mean_test_precision):\n","\n","  if rec > max_recall:\n","    best_model_recall = par\n","    max_recall = rec\n","\n","  if pre > max_precision:\n","    best_model_precision = par\n","    max_precision = pre \n","\n","print('The model with the highest precision had the settings: ', best_model_precision)\n","print('The model with the highest recall had the settings: ', best_model_recall)\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model with the highest precision had the settings:  {'class_weight': None, 'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 20}\n","The model with the highest recall had the settings:  {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 20}\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"yYsFA93tORd3"},"source":["In the code below, we choose the parameter settings that maximize the sum of precision and recall, and tested the models on the testing set."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"7NwhwC1NORd3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667231926925,"user_tz":240,"elapsed":147,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"11884bcc-5b03-4754-8445-485f15750f68"},"source":[" # Find the model with the highest sum of precision and recall\n","params = optimized_dt.cv_results_['params']\n","best_model_params_index = (mean_test_precision + mean_test_recall).argmax()\n","best_model_params = params[best_model_params_index]\n","\n","# Initialize the model \n","best_model = DecisionTreeClassifier(random_state=0, **best_model_params)\n","'''Note, the ** from above allows us to use a dictionary to set \n","the parameters of a function'''\n","\n","# Fit the model on the ENTIRE training set \n","best_model.fit(X_train, y_train)\n","\n","# Evaluate model precision, recall, and score\n","model_precision = precision_score(y_test, best_model.predict(X_test))  # evaluate precision on test set\n","model_recall = recall_score(y_test, best_model.predict(X_test))  # evaluate recall on test set\n","model_score = (model_precision + model_recall) / 2\n","\n","# Add model scores to all_models data frame\n","all_models.loc['CART', 'Grid Search'] = (model_precision, model_recall, model_score, best_model)\n","all_models['Score'].loc['CART', ['Baseline', 'Grid Search']]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["model names  technique  \n","CART         Baseline       0.191177\n","             Grid Search    0.330848\n","Name: Score, dtype: float64"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"eeZYe7zEN1Bc"},"source":["### Exercises\n","> 1. Why does precision and recall get worse after training the model on the training set?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"IzBIPuaVIufr"},"source":["# Clustering\n","The goal of clustering is to bin observations from a dataset into groups of similar points. Unlike regression, clustering is a method that does not require samples of the target data in order to train the model. This is known as _unsupervised learning_ (regression is _supervised learning_). Clustering is often used early on in model engineering to create new features. In this lab, you will cluster data and develop regression models that use the cluster labels as a feature."]},{"cell_type":"markdown","metadata":{"id":"wgh-5cC-XOpa"},"source":["\n","## Distance metrics\n","Clusters are formed from points separated by small distances. Note that there are two types of distance in cluster (1) _Intra_-cluster distance is the distance between points within a cluster, and (2) _inter_-cluster distance is the distance between different clusters. Intra-cluster distance is the metric that $k$-means clustering minimizes, however, there are other methods that use both types of distance (e.g., [agglomerative clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)). The distance is quantified by a chosen _distance measure_ (see Appendix A for a summary of common distance measures).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SKPPeBfcXPNJ"},"source":["## $k$-Means clustering\n","\n","$k$-means clustering is a method for assigning each observation in a given dataset to one of $k$ clusters. For example, you may have a data set of $n$ observations that are indexed by $i = 1, 2, 3... n$ with $m$ features that are indexed by $j = 1, 2, 3... m$. Then we can represent each observation as a point in $m$-D space: $\\mathbf{x}_i = (x_{i1},x_{i2},...,x_{ij})$.\n","\n","$k$-means clustering is a quick and dirty approach to unsupervised learning. It is relatively easy to implement and is generally more efficient than other clustering methods, which makes it useful for large datasets. The main parameter to consider is the number of clusters $k$. The algorithm then finds $k$ optimally-placed cluster centroids. The nearest centroid for a given observation determines the cluster that the observation is binned in.\n","\n","Finding the optimal cluster centroids is an _NP-hard_ problem, and iterative heuristics are consequently used. The most common iterative heuristics is _Lloyds algorithm_, which completes the following steps in each iteration:\n","\n","1. Assign each point to a cluster, based on the nearest centroid.\n","2. Shift each cluster centroid to the centroid of the points in the given cluster.\n","\n","Lloyds algorithm relies on an initial set of centroids (initialized by a random number generator). In practice, randomly generated points that are far apart make for a good set of initial centroids. The algorithm should be run several times to find a good set of centroids.\n"]},{"cell_type":"markdown","metadata":{"id":"cc643Vks0evw"},"source":["## Clustering data\n","Determining the number of clusters to generate often requires exploratory analysis or expert opinion. In simple cases the number of clusters can be intuited once the data is visualized. In general, you should try and play with different numbers of cluster numbers. Use the `KMeans` function to divide the data into 10 separate clusters. Note that you must use the `X_train` for this task."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"R78SibL1Iufs"},"source":["# Define clustering model \n","mdk_k_means = KMeans(n_init = 2,  # number of different centroid seed initializations (number of times algorithm is run)\n","                   n_clusters=10,  # number of clusters (k)\n","                   random_state = 1)  # random seed for k-means algorithm\n","\n","# Fit the model to the training set\n","mdk_k_means.fit(X_train)\n","\n","# Get cluster assignments for each data point\n","clK = mdk_k_means.predict(X_train)\n","\n","# Get the centroid of each cluster\n","Centroids = mdk_k_means.cluster_centers_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"AOS2LP_wIufu"},"source":["### Exercise \n","1. Identify the cluster that stores the most \"educational\" loans."]},{"cell_type":"code","source":["for i in range(10):\n","  print(X_train.Purpose_educational[clK==i].sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00wwEiey0Tlm","executionInfo":{"status":"ok","timestamp":1667232959470,"user_tz":240,"elapsed":150,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"ac3f3937-53fd-4ab2-a629-3a9b1f8025cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","159\n","7\n","0\n","0\n","31\n","0\n","52\n","4\n","0\n"]}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"hauhGfSVIufu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665363942325,"user_tz":240,"elapsed":112,"user":{"displayName":"Bo Lin","userId":"09450153720064869562"}},"outputId":"dbfd529f-2934-4dca-f679-dc3eb7195c6d"},"source":["# Write your code here\n","\n","# -------------------\n","\n","\n","\n","\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cluster 1 has the most, it has 159 education loans\n"]}]},{"cell_type":"markdown","source":["2. Next, change the random state of mdl_k_means to 0. How does changing the random_state effect the cluster with most \"educational\" loans? Why does this happen?"],"metadata":{"id":"K7PfAaCoO3GQ"}},{"cell_type":"code","source":["# -------------------\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# -------------------"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oed1JNf2OxpY","executionInfo":{"status":"ok","timestamp":1665363954943,"user_tz":240,"elapsed":217,"user":{"displayName":"Bo Lin","userId":"09450153720064869562"}},"outputId":"1673b44d-17a1-48a0-cef4-16fddbcbea92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cluster 0 has the most, it has 171 education loans\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"esNVNVNdIufw"},"source":["2. Change the n_init to 100 to run the algorithm 100 times. Compare the `inertia_` attribute of this new model to that of the first clustering model `mdk_k_means`. Why do you think it improved? (**Hint**: Inertia is the sum of squared intra-cluster distances). \n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"DrVcdw6sIufw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667233198745,"user_tz":240,"elapsed":14076,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"7fd8434d-cb2b-489a-ba02-5cd887198acb"},"source":["# Write your code here\n","\n","# -------------------\n","\n","old_inertia = mdk_k_means.inertia_\n","mdk_k_means.n_init = 100\n","mdk_k_means.fit(X_train)\n","new_inertia = mdk_k_means.inertia_\n","percentage = (old_inertia-new_inertia)/old_inertia * 100\n","print('The inertia improved by {:.2f}%'.format(percentage))\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The inertia improved by 0.15%\n"]}]},{"cell_type":"markdown","metadata":{"id":"CIeXawIQBXro"},"source":["In the examples above we arbitrarily set the number of clusters $k$ to 10, however, there are tricks to choosing the right $k$. One common heuristic for choosing the number of clusters is the _elbow method_ (details [here](https://predictivehacks.com/k-means-elbow-method-code-for-python/)), but it's a little controversial. However, since we're planning to use the clusters as a feature in a model (i.e., stacking, which is discussed in the next section), the best thing to do is to see what number of clusters achieves the best score in the resulting stacked model.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"MYIrgcvVIuf3"},"source":["# Ensembles\n","Ensemble methods combine the knowledge from multiple models to achieve better performance than any of the constituent models could achieve alone. "]},{"cell_type":"markdown","metadata":{"id":"rUrgtRowIrQC"},"source":["## Stacking multiple models\n","\n","Stacking is a type of ensemble model that passes the predictions from one or more models as input to another prediction model. In a stacked model, we train a series of *different* prediction models on the dataset. Each of those simple models will predict a target that is then used as a feature(s) for another downstream model. \n","\n","In general, the initial models should do most of the work learning from the data. However, the final model will be able to use those results, and learn when one predictive model is more appropriate than another. `sklearn` actually has a function `StackingClassifier` for stacking models (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)). However, we provide an example of stacking that doesn't use `StackingClassifier` so that we can one-hot-encode the cluster numbers.  \n","\n","Let's create a set of four stacked models that end with our four baseline models and start with a clustering model. "]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ZrEwRU_fIuf3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667233213064,"user_tz":240,"elapsed":6481,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"0cdd9ea2-8adb-4372-f0d9-ce4d79f87ff1"},"source":["'''A stacked model that begins with clustering'''\n","technique_name = 'Stacking'\n","\n","# Initialize the clustering model\n","model = KMeans(n_init=10, n_clusters=6, random_state=0)\n","model.fit(X_train)\n","\n","# Stack training data\n","X_train_prediction = pd.Series(model.predict(X_train), \n","                            name='cluster',\n","                            index=X_train.index)\n","\n","# One-hot-encode cluster numbers\n","X_train_prediction = pd.get_dummies(X_train_prediction)\n","\n","# Stack testing data\n","X_test_prediction = pd.Series(model.predict(X_test),\n","                             name='cluster',\n","                             index=X_test.index)\n","# One-hot-encode cluster numbers\n","X_test_prediction = pd.get_dummies(X_test_prediction)\n","# Get cluster numbers that weren't predicted in the testing set\n","missing_cluster_columns = X_train_prediction.columns.difference(X_test_prediction.columns)\n","# Add missing cluster dummy variables\n","X_test_prediction[missing_cluster_columns] = 0\n","\n","# convert column names from int to str\n","X_train_prediction.columns = [str(c) for c in X_train_prediction.columns]\n","X_test_prediction.columns = [str(c) for c in X_test_prediction.columns]\n","X_test_prediction = X_test_prediction[[c for c in X_train_prediction.columns]]\n","\n","# Add cluster numbers to features -- training\n","X_train_stacked = X_train.join(X_train_prediction)\n","\n","# Add cluster numbers to features -- test\n","X_test_stacked = X_test.join(X_test_prediction)\n","\n","# We will use logistic regression instead of a decision tree at the higher-level classifier\n","all_models = fit_and_score_model(all_models, technique_name, X_train_stacked, X_test_stacked, y_train, y_test)\n","compare_models(technique_name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["LR_L2 achieved a precision of 0.000 and recall of 0.000\n","LR_L1 achieved a precision of 0.259 and recall of 0.585\n","CART achieved a precision of 0.189 and recall of 0.191\n","RF achieved a precision of 0.500 and recall of 0.006\n","On average, scores improved by 0.010, and the most improvement was 0.036\n"]}]},{"cell_type":"markdown","metadata":{"id":"8-9hipLhQF9z"},"source":["### Exercise \n","\n","> 1. Vary the number of clusters in {3, 4, ..., 8} to maximize the biggest improvement between these models and the baseline models. How many clusters do you think we should use in this stacking model?\n"]},{"cell_type":"markdown","metadata":{"id":"sUvSnFvtmELt"},"source":["## Bagging Models\n","Bagging is another type of ensemble model that passes the prediction from one or more models as input to a _voting rule_. For example, random forests are a bagging model that is constructed from a family of simple decision trees (i.e., CART models). Random forests (and ensemble methods in general) work on the assumption that most decision trees will predict well, and they define a simple voting rule (e.g., majority rule) to convert the predictions from several models into a single prediction. Similar to stacking, `sklearn` has a function `VotingClassifer` for bagging models (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html))\n","\n","Throughout this lab we have found that some models clearly performed better than others on both precision and recall. However, even if the average score of one model is less than another, that model may still be better at predicting for certain subsets and scenarios. This is where the idea of ensembles comes in. Below, we use the `VotingClassifer` to fit a bagging model. Note, that we 'weight' the predictions of each model using the model's score, to give better models more sway (i.e., votes) than worse models. "]},{"cell_type":"code","metadata":{"id":"gAJ6W7KQmEjB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667233377644,"user_tz":240,"elapsed":4301,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"761f0ec5-8d14-462a-f2ad-372ea8ed372f"},"source":["# from sklearn.ensemble import VotingClassifier\n","technique_name = 'Bagging'\n","\n","# Get a list of initialized models for bagging\n","models_dict = make_models()\n","# Convert models to list of tuples (required by documentation)\n","model_list = list(models_dict.items()) \n","\n","# Set the weights for each model as their relative performance\n","bagging_weights = all_models.Score[:,'Baseline'].values\n","\n","# Initialize bagging model\n","bagging_model = VotingClassifier(model_list, weights=bagging_weights)\n","# Fit the bagging model (i.e., each of the four models that are bagged)\n","bagging_model.fit(X_train, y_train)\n","\n","# Predict the target using the bagged model\n","bagging_prediction = bagging_model.predict(X_test)\n","\n","# Evaluate ensemble model\n","model_precision = precision_score(y_test,  bagging_prediction)  # evaluate precision on testing set\n","model_recall = recall_score(y_test,  bagging_prediction)  # evaluate recall on testing set\n","model_score = (model_precision + model_recall) / 2\n","\n","# Add ensemble model to all_models dataframe\n","all_models.loc[:, technique_name, :] = (model_precision, model_recall, model_score, None) \n","compare_models(technique_name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On average, scores improved by 0.211, and the most improvement was 0.418\n"]}]},{"cell_type":"markdown","metadata":{"id":"bOVA9RmfY15B"},"source":["### Exercises\n","\n","> 1. You should see that this model was slightly better than the best model from the set of baselines. Why do you think that bagging all of these models was no better than the best of the constituent models?\n","\n",">> We bagged several low quality models (see `all_models` dataframe)\n","\n","> 2. Code up the bagging model above without using the `VotingClassifier`. Your predictions should be identical to what we see above."]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ksHTUa899yhI","executionInfo":{"status":"ok","timestamp":1667235262017,"user_tz":240,"elapsed":153,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"5f4cbec6-b398-4975-faad-578386819b94"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('LR_L2', LogisticRegression(max_iter=200, random_state=0)),\n"," ('LR_L1',\n","  LogisticRegression(class_weight='balanced', max_iter=500, penalty='l1',\n","                     random_state=0, solver='liblinear')),\n"," ('CART', DecisionTreeClassifier(class_weight='balanced', random_state=0)),\n"," ('RF', RandomForestClassifier(class_weight='balanced', random_state=0))]"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"agoFUpKZV1ij"},"source":["# Write your code here\n","\n","# -------------------\n","\n","bag_preds_all = np.zeros((len(y_test), len(model_list)))\n","\n","for i, model in enumerate(model_list):\n","  model[1].fit(X_train, y_train)\n","  model_pred = model[1].predict(X_test)\n","  bag_preds_all[:,i] = model_pred\n","  \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# -------------------\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bagging_prediction = np.sum((bag_preds_all==0) * bagging_weights, 1) < np.sum((bag_preds_all==1) * bagging_weights,1)"],"metadata":{"id":"YCmxUDDr-Zvs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_precision = precision_score(y_test,  bagging_prediction)  # evaluate precision on testing set\n","model_recall = recall_score(y_test,  bagging_prediction)  # evaluate recall on testing set\n","model_score = (model_precision + model_recall) / 2\n","\n","# Add ensemble model to all_models dataframe\n","all_models.loc[:, technique_name, :] = (model_precision, model_recall, model_score, None) \n","compare_models(technique_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyA81hMy_Kxm","executionInfo":{"status":"ok","timestamp":1667235623561,"user_tz":240,"elapsed":150,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"037d367c-7ec4-49dc-b74e-c3f3e2976dc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On average, scores improved by 0.211, and the most improvement was 0.418\n"]}]},{"cell_type":"markdown","metadata":{"id":"L3RWe_sel8Rf"},"source":["# Analyzing the performance of all models\n","\n","Lets finish up by looking at the `all_models` data frame. \n","\n","### Exercise\n","\n","> 1. What model achieved the highest recall, the highest precision, and the highest score?  \n","\n"]},{"cell_type":"code","source":["all_models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":959},"id":"s3--adPQ3q19","executionInfo":{"status":"ok","timestamp":1667233669500,"user_tz":240,"elapsed":690,"user":{"displayName":"Zhuolun Du","userId":"16686697190012944947"}},"outputId":"2713ebad-449f-4067-abaa-57a70a007cfb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 Precision    Recall     Score  \\\n","model names technique                                            \n","LR_L2       Baseline              0.000000  0.000000  0.000000   \n","            Scaling               0.470588  0.016771  0.243680   \n","            Feature Engineering   0.692308  0.018868  0.355588   \n","            Feature_Selection     0.733333  0.023061  0.378197   \n","            Grid Search                NaN       NaN       NaN   \n","            Stacking              0.000000  0.000000  0.000000   \n","            Bagging               0.256506  0.578616  0.417561   \n","LR_L1       Baseline              0.256506  0.578616  0.417561   \n","            Scaling               0.257353  0.587002  0.422178   \n","            Feature Engineering   0.259191  0.591195  0.425193   \n","            Feature_Selection     0.258768  0.572327  0.415547   \n","            Grid Search                NaN       NaN       NaN   \n","            Stacking              0.258813  0.584906  0.421859   \n","            Bagging               0.256506  0.578616  0.417561   \n","CART        Baseline              0.191579  0.190776  0.191177   \n","            Scaling               0.191579  0.190776  0.191177   \n","            Feature Engineering   0.190909  0.176101  0.183505   \n","            Feature_Selection     0.224176  0.213836  0.219006   \n","            Grid Search           0.196286  0.465409  0.330848   \n","            Stacking              0.188797  0.190776  0.189786   \n","            Bagging               0.256506  0.578616  0.417561   \n","RF          Baseline              0.428571  0.006289  0.217430   \n","            Scaling               0.333333  0.004193  0.168763   \n","            Feature Engineering   0.375000  0.006289  0.190645   \n","            Feature_Selection     0.388889  0.014675  0.201782   \n","            Grid Search                NaN       NaN       NaN   \n","            Stacking              0.500000  0.006289  0.253145   \n","            Bagging               0.256506  0.578616  0.417561   \n","\n","                                                                             Model  \n","model names technique                                                               \n","LR_L2       Baseline              LogisticRegression(max_iter=200, random_state=0)  \n","            Scaling               LogisticRegression(max_iter=200, random_state=0)  \n","            Feature Engineering   LogisticRegression(max_iter=200, random_state=0)  \n","            Feature_Selection     LogisticRegression(max_iter=200, random_state=0)  \n","            Grid Search                                                        NaN  \n","            Stacking              LogisticRegression(max_iter=200, random_state=0)  \n","            Bagging                                                           None  \n","LR_L1       Baseline             LogisticRegression(class_weight='balanced', ma...  \n","            Scaling              LogisticRegression(class_weight='balanced', ma...  \n","            Feature Engineering  LogisticRegression(class_weight='balanced', ma...  \n","            Feature_Selection    LogisticRegression(class_weight='balanced', ma...  \n","            Grid Search                                                        NaN  \n","            Stacking             LogisticRegression(class_weight='balanced', ma...  \n","            Bagging                                                           None  \n","CART        Baseline             DecisionTreeClassifier(class_weight='balanced'...  \n","            Scaling              DecisionTreeClassifier(class_weight='balanced'...  \n","            Feature Engineering  DecisionTreeClassifier(class_weight='balanced'...  \n","            Feature_Selection    DecisionTreeClassifier(class_weight='balanced'...  \n","            Grid Search          DecisionTreeClassifier(class_weight='balanced'...  \n","            Stacking             DecisionTreeClassifier(class_weight='balanced'...  \n","            Bagging                                                           None  \n","RF          Baseline             (DecisionTreeClassifier(max_features='auto', r...  \n","            Scaling              (DecisionTreeClassifier(max_features='auto', r...  \n","            Feature Engineering  (DecisionTreeClassifier(max_features='auto', r...  \n","            Feature_Selection    (DecisionTreeClassifier(max_features='auto', r...  \n","            Grid Search                                                        NaN  \n","            Stacking             (DecisionTreeClassifier(max_features='auto', r...  \n","            Bagging                                                           None  "],"text/html":["\n","  <div id=\"df-19d022e5-ac7d-43d4-9930-9826ff9e2baf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th>technique</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">LR_L2</th>\n","      <th>Baseline</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>0.470588</td>\n","      <td>0.016771</td>\n","      <td>0.243680</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>0.692308</td>\n","      <td>0.018868</td>\n","      <td>0.355588</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>0.733333</td>\n","      <td>0.023061</td>\n","      <td>0.378197</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>0.256506</td>\n","      <td>0.578616</td>\n","      <td>0.417561</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">LR_L1</th>\n","      <th>Baseline</th>\n","      <td>0.256506</td>\n","      <td>0.578616</td>\n","      <td>0.417561</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>0.257353</td>\n","      <td>0.587002</td>\n","      <td>0.422178</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>0.259191</td>\n","      <td>0.591195</td>\n","      <td>0.425193</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>0.258768</td>\n","      <td>0.572327</td>\n","      <td>0.415547</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>0.258813</td>\n","      <td>0.584906</td>\n","      <td>0.421859</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>0.256506</td>\n","      <td>0.578616</td>\n","      <td>0.417561</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">CART</th>\n","      <th>Baseline</th>\n","      <td>0.191579</td>\n","      <td>0.190776</td>\n","      <td>0.191177</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>0.191579</td>\n","      <td>0.190776</td>\n","      <td>0.191177</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>0.190909</td>\n","      <td>0.176101</td>\n","      <td>0.183505</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>0.224176</td>\n","      <td>0.213836</td>\n","      <td>0.219006</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>0.196286</td>\n","      <td>0.465409</td>\n","      <td>0.330848</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>0.188797</td>\n","      <td>0.190776</td>\n","      <td>0.189786</td>\n","      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>0.256506</td>\n","      <td>0.578616</td>\n","      <td>0.417561</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">RF</th>\n","      <th>Baseline</th>\n","      <td>0.428571</td>\n","      <td>0.006289</td>\n","      <td>0.217430</td>\n","      <td>(DecisionTreeClassifier(max_features='auto', r...</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>0.333333</td>\n","      <td>0.004193</td>\n","      <td>0.168763</td>\n","      <td>(DecisionTreeClassifier(max_features='auto', r...</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>0.375000</td>\n","      <td>0.006289</td>\n","      <td>0.190645</td>\n","      <td>(DecisionTreeClassifier(max_features='auto', r...</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>0.388889</td>\n","      <td>0.014675</td>\n","      <td>0.201782</td>\n","      <td>(DecisionTreeClassifier(max_features='auto', r...</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>0.500000</td>\n","      <td>0.006289</td>\n","      <td>0.253145</td>\n","      <td>(DecisionTreeClassifier(max_features='auto', r...</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>0.256506</td>\n","      <td>0.578616</td>\n","      <td>0.417561</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19d022e5-ac7d-43d4-9930-9826ff9e2baf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-19d022e5-ac7d-43d4-9930-9826ff9e2baf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-19d022e5-ac7d-43d4-9930-9826ff9e2baf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"LXeIrA6aVNGf","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1665364045937,"user_tz":240,"elapsed":121,"user":{"displayName":"Bo Lin","userId":"09450153720064869562"}},"outputId":"55c00e3b-b6bc-425c-cfc8-92017f777dd8"},"source":["# Write your code here\n","# -------------------\n","\n","\n","\n","\n","\n","\n","\n","# -------------------"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 Precision    Recall     Score  \\\n","model names technique                                            \n","LR_L2       Feature_Selection     0.733333  0.023061  0.378197   \n","LR_L1       Feature Engineering   0.259191  0.591195  0.425193   \n","            Feature Engineering   0.259191  0.591195  0.425193   \n","\n","                                                                             Model  \n","model names technique                                                               \n","LR_L2       Feature_Selection     LogisticRegression(max_iter=200, random_state=0)  \n","LR_L1       Feature Engineering  LogisticRegression(class_weight='balanced', ma...  \n","            Feature Engineering  LogisticRegression(class_weight='balanced', ma...  "],"text/html":["\n","  <div id=\"df-9b74e1b5-7961-42d2-999b-56e160748d3c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th>technique</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>LR_L2</th>\n","      <th>Feature_Selection</th>\n","      <td>0.733333</td>\n","      <td>0.023061</td>\n","      <td>0.378197</td>\n","      <td>LogisticRegression(max_iter=200, random_state=0)</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">LR_L1</th>\n","      <th>Feature Engineering</th>\n","      <td>0.259191</td>\n","      <td>0.591195</td>\n","      <td>0.425193</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>0.259191</td>\n","      <td>0.591195</td>\n","      <td>0.425193</td>\n","      <td>LogisticRegression(class_weight='balanced', ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b74e1b5-7961-42d2-999b-56e160748d3c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9b74e1b5-7961-42d2-999b-56e160748d3c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9b74e1b5-7961-42d2-999b-56e160748d3c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":188}]},{"cell_type":"markdown","metadata":{"id":"S8xtFuv2VNPp"},"source":["> 2. Think about your application. Which models are useful? Are there certain models that are definitively better than others? \n","\n"]},{"cell_type":"markdown","metadata":{"id":"t-A6FDAU1h4G"},"source":["# Closing comments\n","In the end, there are very few rules in terms of what you should and shouldn't do when model engineering. Model engineering is more of an art than a science. Keep in mind that a method that works really well for one application may not work at all for another.  You can add new components in the stack, grid search to find better parameters of the top model, hunt for new features, and train different models on different subsets of data (e.g., bagging/bootstrap). It's even possible to train your models on the prediction errors of other models (e.g., boosting). "]},{"cell_type":"markdown","metadata":{"id":"jxtMlKXx5VvN"},"source":["# Appendix A: Distance measures\n","\n","Let $\\mathbf{x} = (x_1,x_2,\\dots,x_n)$ and $\\mathbf{y} = (y_1,y_2,\\dots,y_n)$ be two observations. A distance metric is defined as a function $d(\\mathbf{x},\\mathbf{y})$, that satisfies the following criteria:\n","\n","1. __Non-negativity:__ $d(\\mathbf{x},\\mathbf{y}) \\geq 0$ and $d(\\mathbf{x},\\mathbf{y}) = 0 \\Rightarrow \\mathbf{x} = \\mathbf{y}$.\n","2. __Symmetry:__ $d(\\mathbf{x},\\mathbf{y}) = d(\\mathbf{y},\\mathbf{x})$.\n","3. __Triangle inequality:__ $d(\\mathbf{x},\\mathbf{y}) + d(\\mathbf{y},\\mathbf{z}) \\geq d(\\mathbf{x},\\mathbf{z})$.\n","\n","Any distance metric can be used for clustering. Moreover, metric-like functions that satisfy some of the constraints---often ignoring the triangle inequality---are also used in practice. In this section, we summarize several commonly used distance functions. Custom distance functions can also be used, however, for most purposes those contained in `pdist` are sufficient.\n","\n","1. __Euclidean (i..e, $l_2$ distance):__ the standard notion of distance that we are most familiar with:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}.$$\n","    \n","2. __Mahalanobis:__ intuitively tries to account for distortions in the data by normalizing with the covariance. Let $K_{i,j}$ be the $i^th$ and $j^{th}$ covariance matrix. Then, the Mahalanobis distance measures:\n","    $$d(\\mathbf{x},\\mathbf{y}) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2 K_{i,j}}.$$\n","    \n","3. __Manhattan/Cityblock (i.e., $l_1$ distance):__ sum of the absolute values of distances:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sum_{i=1}^n | x_i - y_i |.$$\n","\n","4. __Chebychev (i.e., $l_\\infty$ distance):__ largest distance of a single feature:\n","    $$ d(\\mathbf{x}, \\mathbf{y}) = \\max_{i} | x_i - y_i |.$$\n","\n","5. __Minkowski (i.e., $l_p$ distance):__ a generalization of Euclidean, Manhattan, and Chebyshev:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sqrt[p]{\\sum_{i=1}^n |x_i - y_i|^p}.$$\n","\n","6. __Hamming:__ the number of different features between the two vectors:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sum_{i=1}^n \\mathbb{I}(x_i \\neq y_i).$$\n","    "]},{"cell_type":"code","source":[],"metadata":{"id":"qluPfCSB6I4e"},"execution_count":null,"outputs":[]}]}